{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import joblib\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nimport umap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function definition to create a CNN model\ndef create_cnn_model(N, n_features, n_classes, dropout_rate):\n    model = Sequential()\n    model.add(Conv1D(N, kernel_size=3, activation='relu', input_shape=(n_features, 1)))\n    model.add(Dropout(dropout_rate))\n    model.add(BatchNormalization())\n    model.add(MaxPooling1D(pool_size=(N+1)/2))\n    model.add(Conv1D(2*N, kernel_size=3, activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(BatchNormalization())\n    model.add(MaxPooling1D(pool_size=(2*N+1)/2))\n    model.add(Flatten())\n    model.add(Dense(3*N, activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(n_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the training data\ntrain_data = pd.read_csv(\"/kaggle/input/swc-dataset/train_data_swc.csv\")\n\n# Extract the features (X) and target labels (y) from the training data\nX = train_data.drop(\"y\", axis=1)\ny = train_data[\"y\"]\n\n# Load the test data\nX_test = pd.read_csv(\"/kaggle/input/swc-dataset/test_data_swc.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the training data into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n# Standardize the data using the training data's statistics\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = len(y.unique())     # Number of classes: 9\nn_features = X_train.shape[1]   # Number of features: 108","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the parameter grid for hyperparameter tuning\nN = 40\ndropout_rate = 0.3\n\nparams_grid_pipeline = {\n    \"umap__n_components\": [N],\n    \"umap__n_neighbors\": [5, 10, 20],\n    \"umap__min_dist\": [0.1, 0.2],\n}\n\n# Create a KerasClassifier with UMAP-transformed features\numap_model = umap.UMAP()\ncnn_model = KerasClassifier(build_fn=lambda: create_cnn_model(N, n_features, n_classes, dropout_rate), verbose=0)\npipeline = Pipeline([(\"umap\", umap_model),\n                     # (\"reshape\", FunctionTransformer(lambda x: np.reshape(x, (x.shape[0], -1, 1)))),\n                     (\"cnn\", cnn_model)])\ngrid_search = GridSearchCV(estimator=pipeline, param_grid=params_grid_pipeline, cv=StratifiedKFold(n_splits=3, shuffle=True), verbose=2, n_jobs=-1)\n\n# Fit the GridSearchCV to find the best hyperparameters\ngrid_search.fit(X_train, y_train)\n\n# Access the best hyperparameters\nbest_params = grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply UMAP to reduce dimensionality\numap_model = umap.UMAP(n_components=best_params['umap__n_components'],\n                       n_neighbors=best_params['umap__n_neighbors'], min_dist=best_params['umap__min_dist'])\nX_train = umap_model.fit_transform(X_train)\nX_val = umap_model.transform(X_val)\nX_test = umap_model.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the final CNN model with UMAP-transformed features\nfinal_model = create_cnn_model(N, best_params['umap__n_components'], n_classes, dropout_rate)\n\n# Train the final model on the UMAP-transformed data\nfinal_model.fit(X_train, y_train, epochs=10, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create base models (CNNs) as scikit-learn estimators\nbase_models = [\n    (\"cnn1\", KerasClassifier(build_fn=lambda: create_cnn_model(N, best_params['umap__n_components'], n_classes, dropout_rate),\n                             epochs=10, batch_size=32))\n]\n\n# Define the stacking ensemble model\nstacking_model = StackingClassifier(estimators=base_models, final_estimator=LGBMClassifier(n_estimators=30, n_jobs=-1, force_col_wise=True))\n\n# Fit the optimized model on the scaled training data\nstacking_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the optimized model to a file\n# model_filename = \"stacking_model.pkl\"\n# joblib.dump(stacking_model, model_filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the scaled validation set using the optimized model\nval_predictions = stacking_model.predict(X_val)\n\n# Calculate prediction probabilities for validation predictions\nval_proba = stacking_model.predict_proba(X_val)\n\n# Clip predicted probabilities to avoid extremes of the log function\nval_proba = np.clip(val_proba, a_min=1e-15, a_max=1 - 1e-15)\n\n# Calculate log loss for validation predictions\nval_log_loss = log_loss(y_val, val_proba)\nprint(f\"Validation Log Loss: {val_log_loss:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the scaled test data using the optimized model\ntest_predictions = stacking_model.predict(X_test)\n\n# Calculate prediction probabilities for test predictions\ntest_proba = stacking_model.predict_proba(X_test)\n\n# Clip predicted probabilities to avoid extremes of the log function\ntest_proba = np.clip(test_proba, a_min=1e-15, a_max=1 - 1e-15)\n\n# Save the test predictions to a CSV file\nsubmission_df = pd.DataFrame(test_proba, columns=[f\"c{i}\" for i in range(1, n_classes + 1)])\nsubmission_df.to_csv(\"test_predictions.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}