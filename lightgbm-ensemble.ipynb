{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import umap\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.manifold import TSNE\nfrom skopt import BayesSearchCV","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the training data\ntrain_data = pd.read_csv(\"/kaggle/input/swc-dataset/train_data_swc.csv\")\n# Extract the features (X) and target labels (y) from the training data\nX = train_data.drop(\"y\", axis=1)\ny = train_data[\"y\"]\n\n# Load the test data\nX_test = pd.read_csv(\"/kaggle/input/swc-dataset/test_data_swc.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the training data into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\nn_classes = len(y.unique())     # Number of classes: 9\nassert n_classes == 9\nn_features = X_train.shape[1]   # Number of features: 108\nassert n_features == 108","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize the data using the training data's statistics\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform t-SNE transformation\ntsne = TSNE(n_components=3, perplexity=100, random_state=42)\nX_train = tsne.fit_transform(X_train)\nX_val = tsne.fit_transform(X_val)\nX_test = tsne.fit_transform(X_test)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define LightGBM classifier\nlgb_model = LGBMClassifier()\n\n# Define Decision Tree classifier\ndt_model = DecisionTreeClassifier()\n\n# Stack the models using Logistic Regression as the second-level model\nstacked_model = StackingClassifier(\n    estimators=[('lgb', lgb_model), ('decision_tree', dt_model)],\n    final_estimator=LGBMClassifier()\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the parameter space for Bayesian optimization\nparam_space = {\n    'lgb__n_estimators': (50, 100),\n    'lgb__learning_rate': (0.1, 0.2),\n    'lgb__num_leaves': (30, 50),\n    'lgb__max_depth': (5, 10),\n    'decision_tree__max_depth': (5, 10),\n}\n\n# Use StratifiedKFold for cross-validation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Perform hyperparameter tuning with Bayesian optimization\nopt = BayesSearchCV(\n    stacked_model, param_space, cv=cv, n_iter=50, scoring='neg_log_loss', n_jobs=-1, random_state=42\n)\nopt.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = opt.best_params_","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the final model with the best hyperparameters\nfinal_model = stacked_model.set_params(**best_params)\nfinal_model.fit(X_train, y_train)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the validation set\nval_predictions = final_model.predict(X_val)\n\n# Calculate prediction probabilities for validation predictions\nval_proba = final_model.predict_proba(X_val)\n\n# Clip predicted probabilities to avoid extremes of the log function\nval_proba = np.clip(val_proba, a_min=1e-15, a_max=1 - 1e-15)\n\n# Calculate log loss for validation predictions\nval_log_loss = log_loss(y_val, val_proba)\nprint(f\"Validation Log Loss: {val_log_loss:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test data\ntest_predictions = final_model.predict(X_test)\n\n# Calculate prediction probabilities for test predictions\ntest_proba = final_model.predict_proba(X_test)\n\n# Clip predicted probabilities to avoid extremes of the log function\ntest_proba = np.clip(test_proba, a_min=1e-15, a_max=1 - 1e-15)\n\n# Create a DataFrame for test predictions\nsubmission_df = pd.DataFrame(test_proba, columns=[f\"c{i}\" for i in range(1, n_classes + 1)])\n\n# Save the test predictions to a CSV file\nsubmission_df.to_csv(\"test_predictions.csv\", index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}